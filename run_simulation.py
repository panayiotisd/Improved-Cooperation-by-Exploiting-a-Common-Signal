# ***************************************************************************************************************************
# Panayiotis Danassis, Zeki Doruk Erden, and Boi Faltings. 2021.
# Improved Cooperation by Exploiting a Common Signal
# In Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)
# https://arxiv.org/abs/2102.02304
# 
# Requires Ray v. 0.7.5 (https://docs.ray.io/en/master/) and Tensorflow v. 1.14
# ***************************************************************************************************************************

# *** IMPORTS ***
# imports

from fisherman_rllib_randzinit import FishermanEnv
import datetime
import pickle
import os

import numpy as np
import gym, ray
from ray import tune
import argparse
import math

from ray.tune.registry import register_env

from ray.tune.logger import pretty_print

from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ddpg.ddpg import DDPGTrainer
from ray.rllib.agents.a3c.a2c import A2CTrainer
from ray.rllib.agents.a3c.a3c import A3CTrainer

# ***************************************************************************************************************************

# *** FUNCTION DEFINITIONS ***

# Fisherman environment creator function
def create_env_fn(env_context=None):
    return FishermanEnv(
                    n_agents=n_agents, 
                    signal_size=signal_size,
                    signal_duration=signal_duration,
                    f_init=f_init,
                    S_eq=S_eq,
                    growth_rate = growth_rate,
                    max_steps=max_steps)
    

# Log saving function
def save_log(log_dir, n_agents, signal_size, ep_memory):
    stored = {'start_time': start_time, 'n_agents': n_agents, 'signal_size': signal_size, 'episodes': ep_memory}
    file_str = 'N_{0}_k_{1}_Seq_{2}_r_{3}_{4:%H_%M_%S_%d%m%Y}_Episode_Data'.format(n_agents, signal_size, S_eq, growth_rate, start_time)
    path = log_dir + '/' + file_str
    try:
      print("Saving episode logs to: {}".format(path))
      with open(path, "wb") as fp:
          pickle.dump(stored, fp)
    except:
      print("An exception occured while saving episode logs to {}. Continuing program.".format(path))


# policy mapping function
def policy_mapping_fn(agent_id):
    return "agent_policy" + agent_id.split("agent",1)[1]


# Callback function - episode start
def on_episode_start(info):
    # initializations
    episode = info["episode"]
    episode.user_data["observations"] = []
    episode.user_data["actions"] = []

# Callback function - episode step
def on_episode_step(info):
    episode = info["episode"]

    actions = {}
    observations = {}

    # record agent actions and observations
    for agent_id in l_agents:
        actions[agent_id] = episode.last_action_for(agent_id)
        observations[agent_id] = episode.last_observation_for(agent_id)

    # add to episode data
    episode.user_data["actions"].append(actions)
    episode.user_data["observations"].append(observations)

# Callback function - episode end
def on_episode_end(info):
    global ep_number
    global ep_memory
    
    episode = info["episode"]

    # Calculate episode metrics R, U, E, J

    # R: Agent rewards
    R = np.array(list(episode.agent_rewards.values()))
    # U: total reward of all agents
    total_reward = np.sum(R) # named "U" when being appended to episode memory
    # E: Gini coefficient (fairness metric)
    E = np.sum(np.abs(R - np.array([np.roll(R,i) for i in range(R.shape[0])])))
    E /= ((total_reward * 2 * R.shape[0]) if total_reward else 1)
    # J: Jain's index (fairness metric) 
    J = np.sum(R) ** 2 / ( np.sum(R ** 2) * R.shape[0] )

    # get episode action and observation lists saved during steps
    actions = episode.user_data["actions"]
    observations = episode.user_data["observations"]




    # Calculate average actions by signal value
    max_action = 1
    
    average_actions = np.zeros((signal_size,n_agents))

    # do for each signal value
    for k in range(signal_size):
  
        ep_steps = len(actions) # number of steps in episode
        
        # hold averages for each agent
        averages = np.zeros(n_agents)
        
        # timesteps that current signal was observed
        k_timesteps = 0
        
        # loop over timesteps
        for t in range(ep_steps):
                
            # get signal value at current timestep
            obs_all = observations[t]['agent0']
            signal_obs = obs_all[0:signal_size]
            nnz_ind = np.nonzero(signal_obs)
            
            # raise error if there are multiple nonzero entities
            if(len(nnz_ind)!=1):
                raise ValueError('Signal has multiple nonzero entities!')
                
            # check the signal value match
            if(nnz_ind[0] == k):
                
                k_timesteps = k_timesteps+1
                
                # add to average for each agent the action at current signal value k
                for agent in range(n_agents):
            
                    agent_name = 'agent' + str(agent)
                    
                    
                    agent_actions = actions[t][agent_name] # get agent action
                    
                    try:
                        agent_action = agent_actions[0] # only one action present
                    except:
                        agent_action = agent_actions    # possible difference in datatype in some cases


                    # clip between 0 & 1 (maximum value) for plotting ease
                    if(agent_action < 0):
                        agent_action = 0
                    if(agent_action > max_action):
                        agent_action = max_action
                    # add to cumulation variable
                    averages[agent] = averages[agent] + agent_action
                    
        # end of loop over timesteps
                    
        # average accumulated values
        averages = np.divide(averages,k_timesteps)
        
        # set missing values to zero
        # add action values to array - indexing as: (signal value, agent id, episode number)
        for agent in range(n_agents):
            if (math.isnan(averages[agent])): # set NAN values (possible due to 1-step episodes) to previous episode values
                average_actions[k,agent] = 0
            else:
                average_actions[k,agent] = averages[agent]

    # Calculate action variances by signal value
    var_actions = np.zeros((signal_size,n_agents))

    # do for each signal value
    for k in range(signal_size):
        
        ep_steps = len(actions) # number of steps in episode
        
        # hold averages for each agent
        averages = np.zeros(n_agents)
        
        # timesteps that current signal was observed
        k_timesteps = 0
        
        # loop over timesteps
        for t in range(ep_steps):
                
            # get signal value at current timestep
            obs_all = observations[t]['agent0']
            signal_obs = obs_all[0:signal_size]
            nnz_ind = np.nonzero(signal_obs)
            
            # raise error if there are multiple nonzero entities
            if(len(nnz_ind)!=1):
                raise ValueError('Signal has multiple nonzero entities!')
                
            # check the signal value match
            if(nnz_ind[0] == k):
                
                k_timesteps = k_timesteps+1
                
                # add to variance array for each agent the action at current signal value k
                for agent in range(n_agents):
            
                    agent_name = 'agent' + str(agent)
                    
                    
                    agent_action = actions[t][agent_name] # get agent action

                    try:
                        agent_action = agent_action[0] # only one action present
                    except TypeError:
                        agent_action = agent_action    # possible difference in datatype in some cases
                  
                    # clip between 0 & 1 (maximum value) for plotting ease
                    if(agent_action < 0):
                        agent_action = 0
                    if(agent_action > max_action):
                        agent_action = max_action
                    # add to cumulation variable
                    averages[agent] = averages[agent] + np.power((agent_action-average_actions[k,agent]),2)

        # end of loop over timesteps
                    
        # average accumulated values
        averages = np.divide(averages,k_timesteps)
        
     
        # Set missing values to zero & add action values to array - indexing as: (signal value, agent id, episode number)
        for agent in range(n_agents):
            if(math.isnan(averages[agent])): # set NAN values (possible due to 1-step episodes) to previous episode values
                var_actions[k,agent] = 0
            else:
                var_actions[k,agent] = averages[agent]



    # add to episode memory
    if ep_number < n_episodes:
      ep_memory.append({'ep_number': ep_number, 'average_actions':average_actions, 'var_actions':var_actions})
      # save periodically and just before training end
      if (ep_number+1) % 10 == 0 or (ep_number+1) == n_episodes:
        save_log(log_dir, n_agents, signal_size, ep_memory)
    
    ep_number += 1

    episode.custom_metrics["E"] = E
    episode.custom_metrics["J"] = J 
    

# ***************************************************************************************************************************

# *** MAIN CODE ***    

# *** Parse command arguments ***
parser = argparse.ArgumentParser()

# Fundamental arguments
parser.add_argument('--n_agents', required=True, type=int)
parser.add_argument('--signal_size', required=True, type=int)
parser.add_argument('--Ms', required=True, type=float)
parser.add_argument('--hidden_layer_size', default=64, type=int)

# training hyperparameters
parser.add_argument('--lr', default=0.0001, type=float)
parser.add_argument('--gamma', default=0.99, type=float)
parser.add_argument('--lambda_trainer', default=1.0, type=float)

# parallelism parameters
parser.add_argument('--num_workers', default=1, type=int)
parser.add_argument('--num_cpus_per_worker', default=1, type=float)
parser.add_argument('--num_gpus', default=0, type=int)
parser.add_argument('--num_gpus_per_worker', default=0, type=float)

# others
parser.add_argument('--growth_rate', default=1, type=float)
parser.add_argument('--n_episodes', default=5000, type=int)
parser.add_argument('--log_dir', default='./logs')
parser.add_argument('--checkpoint_dir', default='./checkpoints/')
parser.add_argument('--signal_duration', default=1, type=int)
parser.add_argument('--max_steps', default=500, type=int)
parser.add_argument('--train_algo', default="PPO", type=str)

# misc
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--render', action='store_true')
parser.add_argument('--display_frequency', default=100, type=int)
parser.add_argument('--checkpoint_interval', default=50, type=int)

args = parser.parse_args()


# validate GPU
import tensorflow as tf
try:
  device_name = tf.test.gpu_device_name()
  if device_name != '/device:GPU:0':
    raise SystemError('GPU device not found')
  print('Found GPU at: {}'.format(device_name))
except:
  print("Problem finding GPU. Continuing execution.")


# *** Set and get parameters ***

LSH = 0.79 # constant for the limit of sustainable harvesting (K)



# get rest from arguments

# Fundamental arguments
n_agents = args.n_agents
signal_size = args.signal_size
S_eq = args.Ms * LSH * n_agents   
hidden_layer_size = args.hidden_layer_size

# training hyperparameters
lr = args.lr
gamma = args.gamma
lambda_trainer = args.lambda_trainer

# parallelism parameters
num_workers = args.num_workers
num_cpus_per_worker = args.num_cpus_per_worker
num_gpus = args.num_gpus
num_gpus_per_worker = args.num_gpus_per_worker

# others
growth_rate = args.growth_rate
n_episodes = args.n_episodes
log_dir = args.log_dir
checkpoint_dir = args.checkpoint_dir
signal_duration = args.signal_duration
max_steps = args.max_steps
train_algo = args.train_algo

# misc
verbose = args.verbose
render = args.render
display_frequency = args.display_frequency
checkpoint_interval = args.checkpoint_interval



# Common network architecture for all environments
nw_model = {
    # Architecture of hidden layers for fully connected net
    "fcnet_hiddens": [hidden_layer_size, hidden_layer_size],
}  

    
    
# Fixed settings

# Conditions to stopping training - fixed
early_stop_enable = True # set True to enable early stopping based on conditions defined below
cut_eps = 500 # limit of no reward increase and episode length increase, in episodes
max_eplen_ratio = 0.95  # ratio of acceptable maximum episode length
reward_increase_limit = 1.05  # ratio of reward increase limit
reward_decrease_limit = 0.95  # ratio of reward decrease limit     





eplen_limit = max_steps * max_eplen_ratio
f_init = S_eq  # initial resources same as Seq
    


# *** Simulation ***

# Initialize simulation

ep_memory = []
start_time = datetime.datetime.now()

# create log directory
if not os.path.exists(log_dir):
    os.makedirs(log_dir)


# create checkpoint directory
folder_name = 'agents_{0}_k_{1}_Seq_{2}_r_{3}_{4:%H_%M_%S_%d%m%Y}'.format(n_agents, signal_size, S_eq, growth_rate, start_time)
folder_name = folder_name.replace(".", "p")

checkpoint_folder = checkpoint_dir + folder_name
if not os.path.exists(checkpoint_folder):
    os.makedirs(checkpoint_folder)

ep_number = 0 # Current episode
last_save = 0 # Number of episode at last save

# Initialize Ray
ray.shutdown()
ray.init(num_gpus=num_gpus)

# Initialize environment
env_title = "fisherman_env_{}_{}".format(n_agents, signal_size)
print("Environment name: " + env_title)
register_env(env_title, create_env_fn)  # Register environment
env = create_env_fn() # Create environment
obs_space = env.observation_space
act_space = env.action_space

# Initialize agents and policies
num_policies = n_agents
l_agents = ["agent{}".format(i) for i in range(num_policies)]

# Map agent policies
policy_graphs = dict([("agent_policy{}".format(i), (None, obs_space, act_space, {})) 
  for i in range(num_policies)])



# Create trainer depending on input algorithm

if(train_algo == "PPO"):
  
  # Proximal Policy Optimization (PPO)
  print("Training algorithm: Proximal Policy Optimization (PPO)")
  
  trainer = PPOTrainer(
              env=env_title,
              config={
                "num_workers": num_workers,
                "num_cpus_per_worker": num_cpus_per_worker,
                "num_gpus": num_gpus,
                "num_gpus_per_worker": num_gpus_per_worker,
                "model": nw_model,
                "lr": lr,
                "gamma": gamma,
                "lambda": lambda_trainer,
                "multiagent": {
                  "policy_graphs": policy_graphs,
                  "policy_mapping_fn": policy_mapping_fn,
                  "policies_to_train": ["agent_policy{}".format(i) for i in range(n_agents)],
                },
                "callbacks": {
                  "on_episode_start": tune.function(on_episode_start),
                  "on_episode_step": tune.function(on_episode_step),
                  "on_episode_end": tune.function(on_episode_end),
                },
                "log_level": "ERROR",
              })
              
elif(train_algo == "A2C"):
  
  # A2C
  print("Training algorithm: A2C")
  
  trainer = A2CTrainer(
              env=env_title,
              config={
                "num_workers": num_workers,
                "num_cpus_per_worker": num_cpus_per_worker,
                "num_gpus": num_gpus,
                "num_gpus_per_worker": num_gpus_per_worker,
                "model": nw_model,
                "lr": lr,
                "gamma": gamma,
                "lambda": lambda_trainer,
                "multiagent": {
                  "policy_graphs": policy_graphs,
                  "policy_mapping_fn": policy_mapping_fn,
                  "policies_to_train": ["agent_policy{}".format(i) for i in range(n_agents)],
                },
                "callbacks": {
                  "on_episode_start": tune.function(on_episode_start),
                  "on_episode_step": tune.function(on_episode_step),
                  "on_episode_end": tune.function(on_episode_end),
                },
                "log_level": "ERROR",
              })

elif(train_algo == "DDPG"):

  # Deep Deterministic Policy Gradient (DDPG)
  print("Training algorithm: Deep Deterministic Policy Gradient (DDPG)")

  trainer = DDPGTrainer(
              env=env_title,
              config={
                "num_workers": num_workers,
                "num_cpus_per_worker": num_cpus_per_worker,
                "num_gpus": num_gpus,
                "num_gpus_per_worker": num_gpus_per_worker,
                "model": nw_model,
                "lr": lr,
                "gamma": gamma,
                "actor_hiddens": [hidden_layer_size, hidden_layer_size],
                "critic_hiddens": [hidden_layer_size, hidden_layer_size],
                "multiagent": {
                  "policy_graphs": policy_graphs,
                  "policy_mapping_fn": policy_mapping_fn,
                  "policies_to_train": ["agent_policy{}".format(i) for i in range(n_agents)],
                },
                "callbacks": {
                  "on_episode_start": tune.function(on_episode_start),
                  "on_episode_step": tune.function(on_episode_step),
                  "on_episode_end": tune.function(on_episode_end),
                },
                "log_level": "ERROR",
              })
             

elif(train_algo == "A3C"):
  
  # A3C
  print("Training algorithm: A3C ")
  
  trainer = A3CTrainer(
              env=env_title,
              config={
                "num_workers": num_workers,
                "num_cpus_per_worker": num_cpus_per_worker,
                "num_gpus": num_gpus,
                "num_gpus_per_worker": num_gpus_per_worker,
                "model": nw_model,
                "multiagent": {
                  "policy_graphs": policy_graphs,
                  "policy_mapping_fn": policy_mapping_fn,
                  "policies_to_train": ["agent_policy{}".format(i) for i in range(n_agents)],
                },
                "callbacks": {
                  "on_episode_start": tune.function(on_episode_start),
                  "on_episode_step": tune.function(on_episode_step),
                  "on_episode_end": tune.function(on_episode_end),
                },
                "log_level": "ERROR",
              })

else:
  print("Unknown training algorithm")





eplen_max_started = False # flag for early stopping

# Train agent policies until the maximum number of episodes, save checkpoint every checkpoint_interval episodes
checkpoint_counter = 1
episode_rewards_mean = []

# initialize iteration data saving with the guide
file_str = 'N_{0}_k_{1}_Seq_{2}_r_{3}_{4:%H_%M_%S_%d%m%Y}_Iteration_Data'.format(n_agents, signal_size, S_eq, growth_rate, start_time)
f = open(log_dir + "/" + file_str, "a")
f.write("episodes_total - episode_len_mean - policy_reward_mean[policy_i]" + "\n")
f.close()

while ep_number < n_episodes:
  result = trainer.train()
  ep_number = result['episodes_total']
  episode_reward = result['episode_reward_mean']
  episode_len = result['episode_len_mean']
  
  if(early_stop_enable):
    # stopping conditions: close to maximum episode length and no inrease in reward for a number of episodes
    if(episode_len >= eplen_limit):
      
      if(eplen_max_started):  # already started counting
        
        if(episode_reward > eplen_max_base_reward*reward_increase_limit or episode_reward < eplen_max_base_reward*reward_decrease_limit): # if significantly higher or lower reward obtained, restart limit
          eplen_max_base_reward = episode_reward
          eplen_max_start_ep = ep_number
        else: # no higher reward obtained, check stopping condition
          if(ep_number - eplen_max_start_ep > cut_eps):
            break
        
      else: # first time, not yet started counting
        eplen_max_started = True
        eplen_max_base_reward = episode_reward
        eplen_max_start_ep = ep_number

    else:
      eplen_max_started = False

  
  
  print(pretty_print(result))
  episode_rewards_mean.append(result['episode_reward_mean'])
  
  # Uncomment the following to plot the training total rewards progress
  # plt.plot(episode_rewards_mean)
  # plt.show()
  
  
  
  # Save iteration data
  if(result['episodes_this_iter']>0):
      iteration_reward_mean=[str(result['policy_reward_mean'][policy_name])  for policy_name in policy_graphs.keys()]
      iteration_eplen_mean=str(result['episode_len_mean'])
      iteration_episodes_total=str(result['episodes_total'])
      savedata = ""
      savedata += iteration_episodes_total  # first entry is the total number of episodes
      savedata += " " + iteration_eplen_mean   # second entry is the mean episode length
      for pc in iteration_reward_mean:  # other entries are mean policy rewards
        savedata += " "
        savedata += pc
      file_str = 'N_{0}_k_{1}_Seq_{2}_r_{3}_{4:%H_%M_%S_%d%m%Y}_Iteration_Data'.format(n_agents, signal_size, S_eq, growth_rate, start_time)
      f = open(log_dir + "/" + file_str, "a")
      f.write(savedata + "\n")
      f.close()
      

trainer.save(checkpoint_folder) # save checkpoint after final iteration

# Outputs: Final episode rewards and lengths
episode_reward_final = result['episode_reward_mean']
episode_len_final = result['episode_len_mean']    

print("Experiment finished successfully.")

# Create success flag file
success_file_str = 'SUCCESS_N_{0}_k_{1}_Seq_{2}_r_{3}_{4:%H_%M_%S_%d%m%Y}'.format(n_agents, signal_size, S_eq, growth_rate, start_time)
f = open("./" + success_file_str, "a")
f.close()
    
    
    
    
    
    
    
    



