# ***************************************************************************************************************************
# Panayiotis Danassis, Zeki Doruk Erden, and Boi Faltings. 2021.
# Improved Cooperation by Exploiting a Common Signal
# In Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)
# https://arxiv.org/abs/2102.02304
# 
# Requires Ray v. 0.7.5 (https://docs.ray.io/en/master/) and Tensorflow v. 1.14
# ***************************************************************************************************************************

# Fisherman Environment Implementation

import gym
import numpy as np
import random

from gym import spaces
from gym.utils import seeding

from ray.rllib.env.multi_agent_env import MultiAgentEnv


# Environment function definitions
def def_p_fn(cur_stock, cur_step):
    return 1

def def_h_fn(cur_stock, efforts, S_eq):
    q = ( cur_stock ) / (2 * S_eq) if cur_stock <= (2 * S_eq) else 1
    E = np.sum(efforts)
    if E == 0:
        return np.zeros(efforts.shape)
    H = min(cur_stock, q * E)
    
    return H * efforts / E

def def_c_fn(efforts):
    return np.zeros(efforts.shape)

def def_f_fn(cur_stock, growth_rate, S_eq):
    growth = np.exp(growth_rate * (1 - cur_stock / S_eq))
    return cur_stock * growth

def def_s_fn(cur_step, signal_size, signal_duration, signal_stochastic_offset):
    signal = np.zeros(signal_size)
    signal[((cur_step + signal_stochastic_offset) // signal_duration) % signal_size] = 1
    return signal

class FishermanEnv(MultiAgentEnv):
    def __init__(
            self, 
            n_agents, 
            f_init=1,
            growth_rate=1,
            S_eq=1, 
            max_steps=1000, 
            signal_size=2, 
            signal_duration=1,
            threshold=1e-4,
            h_fn=def_h_fn,
            f_fn=def_f_fn,
            p_fn=def_p_fn,
            c_fn=def_c_fn,
            s_fn=def_s_fn
            ):
        """
        Parameters
        ----------
        n_agents : int
            Number of agents in the environment
        f_init : double
            Initial resource stock 
        growth_rate : double
            Intrinsic growth rate 
        S_eq : double
            Equilibrium population
        max_steps : int
            Maximum number of steps
        signal_size : int
            Signal size (cardinality)
        signal_duration : int
            Duration between signal shifts
        threshold : double
            Minimum stock thresold
        h_fn : lambda
            Total consumed resources function
        f_fn : lambda
            Spwaner-recruit function
        p_fn : lambda
            Price function
        c_fn : lambda
            Cost function
        s_fn : lambda
            Signal function

        """
        super(FishermanEnv, self).__init__()

        self.n_agents = n_agents
        self.max_steps = max_steps
        self.f_init = f_init
        self.growth_rate = growth_rate

        self.S_eq = S_eq

        self.signal_size = signal_size
        self.signal_duration = signal_duration
        self.signal_stochastic_offset = random.randint(1,signal_size)

        self.done = True
        self.cur_stock = None
        self.cur_step = None
        self.last_actions = None

        self.threshold = threshold

        self.seed()

        self.l_agents = ["agent{}".format(i) for i in range(self.n_agents)]

        # Environment specific customs
        self.h_fn = h_fn
        self.f_fn = f_fn
        self.p_fn = p_fn
        self.c_fn = c_fn
        self.s_fn = s_fn

        self.observation_space = spaces.Tuple((spaces.MultiBinary(self.signal_size), spaces.Box(low=0, high=1, shape=(1,), dtype=float), spaces.Box(low=0, high=np.inf, shape=(1,), dtype=float)))
        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=float)


    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]


    def step(self, actions):
        """
        Perform one step in the enviroment

        Parameters
        ----------
        actions : array(double)
            Agent actions
        """
        if self.done:
            return None

        a_actions = np.zeros(self.n_agents)
        for i, v in enumerate(self.l_agents):
            if v in actions:
                a_actions[i] = actions[v]

        efforts = self._get_efforts(a_actions)
        efforts[efforts < 0] = 0

        price = self.p_fn(self.cur_stock, self.cur_step)
        Hs = self.h_fn(self.cur_stock, efforts, self.S_eq)
        cs = self.c_fn(efforts)
        

        consumed = np.sum(Hs)
        rewards = price * Hs - cs
        remaining = max(0, self.cur_stock - consumed)
        self.cur_stock = self.f_fn(remaining, self.growth_rate, self.S_eq)


        signal = self.s_fn(self.cur_step, self.signal_size, self.signal_duration, self.signal_stochastic_offset)

        self.cur_step += 1
        self.last_actions = a_actions
        self.last_rewards = rewards
        self.done = self.cur_stock <= self.threshold or self.cur_step >= self.max_steps
        self.last_efforts = efforts

        states = {}
        rewards = {}
        dones = {}
        infos = {}

        for i, v in enumerate(self.l_agents):
            states[v] = (signal, np.array([self.last_efforts[i]]), np.array([self.last_rewards[i]]))
            rewards[v] = self.last_rewards[i]
            dones[v] = self.done
        dones['__all__'] = self.done

        return states, rewards, dones, infos


    def reset(self):
        self.cur_stock = self.f_init
        self.cur_step = 0
        self.signal_stochastic_offset = random.randint(1,self.signal_size)

        signal = self.s_fn(self.cur_step, self.signal_size, self.signal_duration, self.signal_stochastic_offset)

        states = {}

        self.done = False
        self.last_actions = np.zeros(self.n_agents)
        self.last_rewards = np.zeros(self.n_agents)
        self.last_efforts = np.zeros(self.n_agents)

        for i, v in enumerate(self.l_agents):
            states[v] = (signal, np.array([self.last_actions[i]]), np.array([self.last_rewards[i]]))

        return states


    def render(self):
        print("Current stock: %d" % (self.cur_stock))

    def close(self):
        return

    def _get_efforts(self, actions):
        efforts = np.clip(actions, 0, 1)
        return efforts

